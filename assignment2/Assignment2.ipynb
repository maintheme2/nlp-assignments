{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nordvig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('data/big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]).union(known(edits1(word))) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spell corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "bigrams = open('data/bigrams.txt', 'r', encoding='latin-1')\n",
        "fivegrams = open('data/fivegrams.txt', 'r')\n",
        "\n",
        "two_grams = {}\n",
        "for line in bigrams:\n",
        "    words = line.strip().split('\\t')\n",
        "    \n",
        "    two_grams[tuple(words[1:])] = int(words[0])\n",
        "\n",
        "\n",
        "three_grams = {}\n",
        "four_grams = {}\n",
        "five_grams = {}\n",
        "for line in fivegrams:\n",
        "    words = line.strip().split('\\t')\n",
        "\n",
        "    freq = int(words[0])\n",
        "    \n",
        "    five_grams[tuple(words[1:])] = freq\n",
        "    \n",
        "    for gram in [words[1:-1], words[2:]]:\n",
        "        if tuple(gram) in four_grams: four_grams[tuple(gram)] += freq\n",
        "        else: four_grams[tuple(gram)] = freq\n",
        "    \n",
        "    for gram in [words[1:4], words[2:5], words[3:]]:\n",
        "        if tuple(gram) in three_grams: three_grams[tuple(gram)] += freq\n",
        "        else: three_grams[tuple(gram)] = freq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "grams = [None, None, two_grams, three_grams, four_grams, five_grams]\n",
        "\n",
        "def find_word_in_context_by_ngrams(ngrams, word, context):\n",
        "\n",
        "    '''\n",
        "    Find word in context by ngrams\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ngrams : list\n",
        "        List of n-grams\n",
        "    word : str\n",
        "        Word\n",
        "    context : list\n",
        "        Context of the word\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        List of contexts\n",
        "    '''\n",
        "    \n",
        "    # len of context can at most be 5\n",
        "    context = context[-5:]\n",
        "    \n",
        "    context.append(word)\n",
        "    to_return = defaultdict(list)\n",
        "    for ind, ngram in enumerate(ngrams):\n",
        "        if not ngram: continue\n",
        "\n",
        "        # sliced part of the context based on current ngram\n",
        "        context_slice = tuple(context[-ind:])\n",
        "        if context_slice in ngram.keys():\n",
        "            # add current slice to the list of contexts\n",
        "            if not to_return[context_slice]: \n",
        "                to_return[context_slice] = ngram[context_slice]\n",
        "            else: \n",
        "                to_return[context_slice] += ngram[context_slice]\n",
        "    \n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list, {('babe', 'in'): 73, ('a', 'babe', 'in'): 16})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_word_in_context_by_ngrams(grams, 'in', ['a', 'babe'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_best_word(list_of_contexts):\n",
        "\n",
        "    '''\n",
        "    Get the best word from the contexts of all candidates\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    list_of_contexts : list\n",
        "        List of contexts\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Best word out of all contexts\n",
        "    '''\n",
        "\n",
        "    max_value = 0 # number of occurrences of the context\n",
        "    best_word = None # best word out of all contexts\n",
        "    max_gram_len = 0 # length of the best ngram\n",
        "    for context in list_of_contexts:\n",
        "        if len(context) == 0: continue \n",
        "        \n",
        "        for gram in context:\n",
        "            # take the largest context possible\n",
        "            if max_gram_len <= len(gram):\n",
        "                # if len of the current ngram is larger than definately update the max value\n",
        "                if max_gram_len < len(gram):\n",
        "                    max_value = context[gram]\n",
        "                # else update the max value only if it is larger\n",
        "                else:\n",
        "                    if max_value < context[gram]:\n",
        "                        max_value = context[gram]\n",
        "\n",
        "                max_gram_len = len(gram)\n",
        "                \n",
        "                # and also choose the best word in that context\n",
        "                best_word = gram[-1]\n",
        "\n",
        "    return best_word\n",
        "\n",
        "def correct_word(grams, word, context):\n",
        "\n",
        "    '''\n",
        "    Make a correction of given misspelled word.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    grams : list\n",
        "        List of n-grams\n",
        "    word : str\n",
        "        Misspelled word\n",
        "    context : list\n",
        "        Context of the word\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Corrected word\n",
        "    '''\n",
        "\n",
        "    # create a list of candidates\n",
        "    cands = candidates(word)\n",
        "\n",
        "    # get the contexts of all candidates\n",
        "    list_of_contexts = [find_word_in_context_by_ngrams(grams, c, context) for c in cands]\n",
        "\n",
        "    flag = False\n",
        "    for c in list_of_contexts:\n",
        "        if len(list(c.items())) > 0:\n",
        "            flag = True\n",
        "            break\n",
        "    \n",
        "    # just return the best candidate if there is no context for any candidate\n",
        "    if not flag: \n",
        "        return max(cands, key=P)\n",
        "\n",
        "    return get_best_word(list_of_contexts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "*Your text here...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My accuracy 0.6975\n",
            "Nordvig accuracy 0.69125\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "def spoil_word(word):\n",
        "    # use nordvig edit1 to spoil the word\n",
        "    spoiled_word = list(edits1(word))[0]\n",
        "    return spoiled_word\n",
        "\n",
        "file = open('data/english.txt', 'r')\n",
        "\n",
        "counter_mine = 0\n",
        "counter_nordvig = 0\n",
        "file_len = 0\n",
        "for line in file:\n",
        "\n",
        "    spoiled_word_index = random.randint(1, len(line.split()) - 1)\n",
        "    spoiled_word = spoil_word(line.split()[spoiled_word_index])\n",
        "\n",
        "    context = line.split()[0:spoiled_word_index]\n",
        "    \n",
        "    corrected_word = correct_word(grams, spoiled_word, context)\n",
        "    if corrected_word == line.split()[spoiled_word_index]:\n",
        "        counter_mine += 1\n",
        "    \n",
        "    nordvig_corrected_word = correction(spoiled_word)\n",
        "    if nordvig_corrected_word == line.split()[spoiled_word_index]:\n",
        "        counter_nordvig += 1\n",
        "    \n",
        "    file_len += 1\n",
        "\n",
        "print('My accuracy', counter_mine/file_len)\n",
        "print('Nordvig accuracy', counter_nordvig/file_len)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
